# -*- coding: utf-8 -*-
"""Projet de Machine Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ik2oEfa9tkYfIFtGXt7oN7LmZO6gQyY1
"""

import pandas as pd

!pip install kaggle

!kaggle datasets download sureshmecad/mens-shoe-prices

!unzip mens-shoe-prices.zip

df = pd.read_csv('train.csv')

df.head()

df.info()

df.shape

df.isnull().sum()

null_percentage = df.isnull().sum() / len(df) * 100
null_percentage_sorted = null_percentage.sort_values(ascending=False)
null_percentage_sorted

columns_to_drop = null_percentage[null_percentage > 40].index
df1 = df.drop(columns=columns_to_drop)

df1.head()

unique_values = df1['categories'].unique()
unique_values

df1.drop(columns=['id', 'dateadded', 'dateupdated',
                 'features', 'imageurls', 'keys',
                 'manufacturernumber', 'merchants', 'prices_condition',
                 'prices_dateadded', 'prices_dateseen', 'prices_issale',
                 'prices_merchant','prices_sourceurls', 'sourceurls'],
       inplace=True)

df1.isnull().sum()

df1.shape

df1.columns

df1.info()

df1['prices_amountmin'] = df1['prices_amountmin'].fillna(0)

df1['prices_amountmin'] = pd.to_numeric(df1['prices_amountmin'], errors='coerce')

df1['prices_amountmax'] = df1['prices_amountmax'].replace({'\$': '', ',': ''}, regex=True)
df1['prices_amountmax'] = pd.to_numeric(df1['prices_amountmax'], errors='coerce')

df1.dtypes

df1['price_average'] = (df1['prices_amountmin'] + df1['prices_amountmax']) / 2

df1.drop(columns=['prices_amountmin', 'prices_amountmax'], inplace=True)

df1.columns

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df1['brand_encoded'] = le.fit_transform(df1['brand'])
df1['categories_encoded'] = le.fit_transform(df1['categories'])
df1['name_encoded'] = le.fit_transform(df1['name'])
df1['prices_currency_encoded'] = le.fit_transform(df1['prices_currency'])

df1.describe()

df1.drop(columns=['brand', 'categories', 'name', 'prices_currency'], inplace=True)

df1['price_average'] = df1['price_average'].fillna(0)

df1.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.boxplot(data=df1)
plt.xticks(rotation=90)
plt.title('Boxplots of the data')
plt.show()

Q1 = df1.quantile(0.25)
Q3 = df1.quantile(0.75)
IQR = Q3 - Q1

# Filter out outliers
df1 = df1[~((df1 < (Q1 - 1.5 * IQR)) | (df1 > (Q3 + 1.5 * IQR))).any(axis=1)]

plt.figure(figsize=(10, 6))
sns.boxplot(data=df1)
plt.xticks(rotation=90)
plt.title('Boxplots of the data')
plt.show()

df1.info()

import matplotlib.pyplot as plt
import seaborn as sns

# List of columns to plot
columns = [
    'price_average', 'brand_encoded', 'categories_encoded', 'name_encoded',
       'prices_currency_encoded'
]

# Set up a plotting grid
n_cols = 3
n_rows = (len(columns) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))

# Flatten axes for easy iteration
axes = axes.flatten()

# Plotting loop for histograms and bar plots
for idx, col in enumerate(columns):
    ax = axes[idx]
    if df1[col].dtype in ['int64', 'float64']:
        # If numeric, plot histogram
        ax.hist(df1[col].dropna(), bins=20, color='green', alpha=0.7)
        ax.set_title(f'Histogram of {col}')
        ax.set_xlabel(col)
        ax.set_ylabel('Frequency')
    else:
        # If categorical, plot bar plot
        value_counts = df[col].value_counts()
        ax.bar(value_counts.index.astype(str), value_counts.values, color='orange', alpha=0.7)
        ax.set_title(f'Bar Plot of {col}')
        ax.set_xlabel(col)
        ax.set_ylabel('Count')
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

# Remove empty subplots if any
for j in range(idx + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df1['price_scaled'] = scaler.fit_transform(df1[['price_average']])

df1.columns

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Now you can access columns by name using .loc
X = df1.loc[:, ['brand_encoded', 'categories_encoded', 'name_encoded', 'prices_currency_encoded']]
y = df1['price_average']


# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)

# Initialize and train the Decision Tree Regressor model
model = DecisionTreeRegressor(random_state=50)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = mse ** 0.5
r2 = r2_score(y_test, y_pred)

# Print the results
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')

"""R_squared is a statistical measure in a regression model. So, using this measure in the Decision Tree Regressor, we can say that only 43,43% of the variance of the dependent variable can be explained by the independent variable."""

import numpy as np
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# 4. Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)

# 5. Build the Neural Network Model using Keras
model = Sequential()

# Input Layer and First Hidden Layer
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))

# Second Hidden Layer
model.add(Dense(32, activation='relu'))

# Output Layer (for regression problem, use linear activation)
model.add(Dense(1, activation='linear'))

# 6. Compile the model
model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['mae'])

# 7. Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# 8. Make predictions
y_pred = model.predict(X_test)

# 9. Evaluate the model performance
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')

"""Using neural networks model, R_squared is about 0,74% which means that this model can not be the best to use in this case, because the variance of the dependent variable can not be really explained by the independent variable."""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)

# Create the Linear Regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print performance metrics
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')

"""R-squared = 0.0074 is very low, meaning that the model can not exaplain the variance in the target variable, which means that the model does not perform well."""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)

# Create the Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=50)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print performance metrics
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')

"""R-squared = 58,81% is a good indicator, meaning your model explains more than 58,81% of the variance in the target variable, which means that the model performs well. That is why we will opt for Random Forest Regressor model."""

import pickle
rf=RandomForestRegressor(n_estimators=100,random_state=50)
# Save the trained model to a file
with open('mens-shoe-prices.pkl', 'wb') as model_file:
    pickle.dump(rf, model_file)  # 'rf' is the trained Random Forest model

!pip install flask

from flask import Flask, request, jsonify
import pickle
import numpy as np

# Load the model
with open('mens-shoe-prices.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

# Initialize the Flask app
app = Flask(__name__)

# Define prediction route
@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()  # Get the data from POST request

    # Convert input data to a numpy array or pandas DataFrame
    input_data = np.array([data['input_features']])

    # Make prediction using the model
    prediction = model.predict(input_data)

    # Return the result as JSON
    result = {'prediction': prediction[0]}
    return jsonify(result)

if __name__ == '__main__':
    app.run(debug=True)

"""I have chosen a dataset from Kaggle which is about mens' shoe prices, because I wanted to see the different features that can affect the price of this product. So, first of all, I have seen the dataframe columns, description and info. Then, I have cleaned the data, by dealing with null values, deleting the columns that are not really important in the modeling phase. After that, I have encoded the categorical features that will be the base of our model.
I have also used feature engineering by creatin a new column wich is 'price average' which is the result of this operation:

df1['price_average'] = (df1['prices_amountmin'] + df1['prices_amountmax']) / 2

After that, I have eliminated outliers using quartiles:

Q1 = df1.quantile(0.25)
Q3 = df1.quantile(0.75)
IQR = Q3 - Q1

df1 = df1[~((df1 < (Q1 - 1.5 * IQR)) | (df1 > (Q3 + 1.5 * IQR))).any(axis=1)]

I have also used standard scaler on price_avarage, before trying the different models which are :
* Decision Tree Regressor
* Neural networks
* Linear regression
* Random Forest Regressor.

Finally, I have found that the Random Forest Regressor which fils better our data.
"""